{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4abe7d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6362099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame, functions as F\n",
    "\n",
    "from src.innova.config.config import etl_config\n",
    "from utils.trackers import setup_logging, save_metadata\n",
    "from utils.spark_helpers import (\n",
    "    load_parquet, save_parquet, load_metadata, save_metadata\n",
    ")\n",
    "from utils.transform_schema import apply_strict_schema\n",
    "from utils.connections.sql_server_connector import SQLServerConnector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d467a41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14888435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/alegra/metadata/innova/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUSINESS: str = \"innova\"\n",
    "LOAD_TS = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "LAYER: str = etl_config[\"etl\"][\"layer_slv\"]\n",
    "STEP_NAME_1: str = etl_config[\"etl\"][\"steps\"][0][\"name\"]\n",
    "STEP_NAME_2: str = etl_config[\"etl\"][\"steps\"][1][\"name\"]\n",
    "BASE_PATH: str = etl_config[\"paths\"][\"base_path\"]      \n",
    "LOGS_PATH: str = etl_config[\"paths\"][\"logs_path\"]    \n",
    "METADATA_PATH: str = etl_config[\"paths\"][\"metadata_path\"] \n",
    "\n",
    "BASE_SILVER: Path = f\"{BASE_PATH}/{LAYER}/{BUSINESS}\"\n",
    "\n",
    "\n",
    "logger = setup_logging(LOGS_PATH)\n",
    "logger.info(f\"starting {STEP_NAME_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cca380a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'customers': {'path': '/alegra/data_lake/bronze/innova/customers/load_date=20250601055858',\n",
       "  'load_date': '20250601055858',\n",
       "  'record_count': 100},\n",
       " 'products': {'path': '/alegra/data_lake/bronze/innova/products/load_date=20250601055858',\n",
       "  'load_date': '20250601055858',\n",
       "  'record_count': 50},\n",
       " 'invoices': {'path': '/alegra/data_lake/bronze/innova/invoices/load_date=20250601055858',\n",
       "  'load_date': '20250601055858',\n",
       "  'record_count': 1000}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_file_step_01 = Path(f\"{METADATA_PATH}/{STEP_NAME_1}.json\")\n",
    "logger.info(f\"Loading Bronze metadata from: {metadata_file_step_01}\")\n",
    "bronze_metadata = load_metadata(\n",
    "    metadata_path=metadata_file_step_01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40636d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bronze_df(\n",
    "        spark: SparkSession, \n",
    "        bronze_path: str, \n",
    "        logger: logging.Logger\n",
    ") -> DataFrame:\n",
    "    logger.info(f\"Loading Bronze parquet from: {bronze_path}\")\n",
    "    return load_parquet(\n",
    "        spark=spark, \n",
    "        path=bronze_path, \n",
    "        logger=logger\n",
    "    )\n",
    "\n",
    "\n",
    "def transform_and_cast(\n",
    "    source_name: str,\n",
    "    df: DataFrame,\n",
    "    schema_dict: dict,\n",
    "    logger: logging.Logger\n",
    ") -> DataFrame:\n",
    "    logger.info(f\"Transforming and casting table: {source_name}\")\n",
    "    if source_name == \"customers\":\n",
    "        df = df.filter(F.col(\"ID\").isNotNull())\n",
    "    elif source_name == \"products\":\n",
    "        df = df.filter(F.col(\"ID\").isNotNull())\n",
    "    elif source_name == \"invoices\":\n",
    "        df = (\n",
    "            df.filter(F.col(\"ID\").isNotNull())\n",
    "              .filter(F.col(\"ClienteID\").isNotNull())\n",
    "              .filter(F.col(\"ProductoID\").isNotNull())\n",
    "        )\n",
    "    df = apply_strict_schema(df, schema_dict)\n",
    "    count_after = df.count()\n",
    "    logger.info(f\"[{source_name}] Row count after cast: {count_after}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def write_to_silver_parquet(\n",
    "    df: DataFrame,\n",
    "    silver_path: str,\n",
    "    logger: logging.Logger\n",
    ") -> None:\n",
    "    logger.info(f\"Saving to Silver parquet: {silver_path}\")\n",
    "    save_parquet(data=df, path=silver_path, logger=logger)\n",
    "\n",
    "\n",
    "def write_to_sql_staging(\n",
    "    spark: SparkSession,\n",
    "    df: DataFrame,\n",
    "    staging_table: str,\n",
    "    logger: logging.Logger\n",
    ") -> None:\n",
    "    logger.info(f\"Writing to SQL Server staging: {staging_table}\")\n",
    "    with SQLServerConnector(spark, logger=logger) as sql_conn:\n",
    "        sql_conn.write_dataframe(\n",
    "            df,\n",
    "            table=staging_table,\n",
    "            mode=\"overwrite\",\n",
    "            options={\"batchsize\": 1000}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3184249c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customers customers_stg {'ID': 'LongType', 'Nombre': 'StringType', 'Ubicacion': 'StringType', 'Segmento': 'StringType'}\n",
      "products products_stg {'ID': 'LongType', 'Nombre': 'StringType', 'Categoria': 'StringType', 'Precio': 'DecimalType(18,14)'}\n",
      "invoices invoices_stg {'ID': 'LongType', 'Fecha': 'DateType', 'ClienteID': 'LongType', 'ProductoID': 'LongType', 'Cantidad': 'LongType', 'Total': 'DecimalType(18,14)'}\n"
     ]
    }
   ],
   "source": [
    "metadata_silver: dict = {}\n",
    "tables_cfg = etl_config[\"tables\"]\n",
    "\n",
    "for table_key, table_conf in tables_cfg.items():\n",
    "    source_name = table_conf[\"source\"]\n",
    "    target_name = table_conf[\"target\"]\n",
    "    schema_dict = table_conf[\"schema\"]\n",
    "    print(\n",
    "       source_name,\n",
    "       target_name,\n",
    "       schema_dict\n",
    "    )\n",
    "\n",
    "    bronze_info = bronze_metadata.get(source_name)\n",
    "    if bronze_info is None:\n",
    "        logger.error(f\"No Bronze metadata for '{source_name}', skipping.\")\n",
    "        continue\n",
    "\n",
    "    df_bronze = load_bronze_df(\n",
    "        spark=spark, \n",
    "        bronze_path=bronze_info[\"path\"], \n",
    "        logger=logger\n",
    "    )\n",
    "\n",
    "    df_transformed = transform_and_cast(\n",
    "        source_name=source_name, \n",
    "        df=df_bronze, \n",
    "        schema_dict=schema_dict, \n",
    "        logger=logger\n",
    "    )\n",
    "\n",
    "    silver_path = f\"{BASE_SILVER}/temp_tables/{target_name}\"\n",
    "    write_to_silver_parquet(\n",
    "        df=df_transformed, \n",
    "        silver_path=silver_path, \n",
    "        logger=logger\n",
    "    )\n",
    "\n",
    "    db_table_full = f\"silver_prod.temp_tables.{target_name}\"\n",
    "    write_to_sql_staging(\n",
    "        spark=spark, \n",
    "        df=df_transformed, \n",
    "        staging_table=db_table_full, \n",
    "        logger=logger\n",
    "    )\n",
    "\n",
    "    record_count = df_transformed.count()\n",
    "    metadata_silver[table_key] = {\n",
    "        \"path\"        : silver_path,\n",
    "        \"load_date\"   : LOAD_TS,\n",
    "        \"record_count\": record_count,\n",
    "        \"db_table\"    : db_table_full\n",
    "    }\n",
    "\n",
    "logger.info(f\"Finished {STEP_NAME_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "029e808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_metadata_file = Path(f\"{METADATA_PATH}/{STEP_NAME_2}.json\")\n",
    "logger.info(f\"Writing Silver metadata to: {output_metadata_file}\")\n",
    "save_metadata(\n",
    "    metadata=metadata_silver, \n",
    "    metadata_path=output_metadata_file\n",
    ")\n",
    "logger.info(\"Metadata for Step 02 saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
