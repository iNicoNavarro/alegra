# Soluci√≥n de Arquitectura de Datos para el Desaf√≠o T√©cnico Alegra

Este repositorio presenta una soluci√≥n completa al desaf√≠o t√©cnico de un Ingeniero de Datos, ambientado en un entorno de simulaci√≥n profesional. La arquitectura implementada emula un ecosistema productivo compuesto por un Data Warehouse modelado en esquema **copo de nieve**, y un pipeline ETL modular construido en **PySpark**, ejecutado dentro de un entorno **Dockerizado**.

### Contexto del Entorno Alegra

El entorno simulado representa una arquitectura tipo **Lakehouse**, caracterizada por zonas de almacenamiento diferenciadas (`bronze`, `silver`, `gold`). Adicionalmente, se incluye un contenedor de **SQL Server** que funge como el sistema transaccional OLTP de origen.

Es importante destacar que los datos de entrada no se obtienen directamente del contenedor SQL, sino que se alojan inicialmente en la carpeta `data_source/`. Desde esta ubicaci√≥n, los datos atraviesan las siguientes etapas en el pipeline:

1. **Zona Bronze**: Los datos se cargan sin transformaciones, manteniendo su estructura original.
2. **Zona Silver**: Se aplican procesos de limpieza, normalizaci√≥n y modelado de datos conforme al esquema **Copo de Nieve**.
3. **Zona Gold**: Se generan las vistas anal√≠ticas o tablas agregadas necesarias para responder a las **m√©tricas de negocio** solicitadas en la prueba t√©cnica.

Este enfoque promueve una clara separaci√≥n entre los niveles de calidad de los datos, garantiza la **trazabilidad** y facilita la **escalabilidad** futura mediante la integraci√≥n con cat√°logos de datos externos, motores SQL o herramientas de Business Intelligence.

---

# Consideraciones T√©cnicas y Respuestas al Desaf√≠o

* El m√≥dulo `src/` fue dise√±ado para soportar m√∫ltiples negocios. Actualmente se implementa el flujo para `innova`, pero es totalmente **escalable**: basta con a√±adir otra subcarpeta por negocio y configurar su `etl_config.yml`.
* El pipeline **no se dise√±√≥ como pseudoc√≥digo**, sino como un **c√≥digo funcional y preparado para producci√≥n**, soportando orquestaci√≥n por pasos (`step_01` a `step_03`), carga incremental y trazabilidad total mediante metadatos y logs persistentes.
* La complejidad del modelo se refleja no solo en su dise√±o copo de nieve, sino en la infraestructura implementada:

  * Procesamiento con **Spark**.
  * Carga final a un modelo en **SQL Server** embebido en contenedor, incluyendo configuraci√≥n de **drivers JDBC**.
  * Implementaci√≥n de clases reutilizables que automatizan el `upsert` con plantillas `MERGE`.
* **Plan de ETL:**

  1. **Ingesta cruda** desde `data_source/` ‚Üí zona `bronze`.
  2. **Transformaci√≥n estructural** y casting ‚Üí `silver` (staging).
  3. **Carga del DWH**: poblar dimensiones y hechos respetando orden de dependencias.
  4. **Consultas anal√≠ticas** sobre zona `gold` con m√©tricas definidas por la prueba.
* **Innovaci√≥n:** uso de Spark local en un entorno aislado (contenedor Jupyter + Spark + SQLServer), demostrando habilidades pr√°cticas de despliegue y desarrollo modular.

---

### Requisitos del Entorno

Para ejecutar la soluci√≥n propuesta, aseg√∫rese de tener instaladas las siguientes herramientas:

* Python 3.10+
* pip
* Virtualenv (`python -m venv`)
* Docker (para la instancia simulada de SQL Server)
* PySpark (utilizado en los notebooks)
* JupyterLab o VSCode (para desarrollo con notebooks)
* Java Development Kit (JDK)

### Configuraci√≥n del Entorno

Siga estos pasos para configurar y preparar el entorno de desarrollo:

1. **Clonar el Repositorio:**

   ```bash
   git clone https://github.com/usuario/data-engineer-health-assessment.git
   cd data-engineer-health-assessment
   ```
2. **Crear y Activar el Entorno Virtual:**

   ```bash
   python -m venv venv
   source venv/bin/activate  # Para Linux/macOS
   # Para Windows: .\venv\Scripts\activate
   ```
3. **Instalar Dependencias:**

   ```bash
   pip install -r requirements.txt
   ```
4. **Instalar OpenJDK 11 (Recomendado para Spark 3.x):**

   ```bash
   sudo apt update
   sudo apt install openjdk-11-jdk -y
   ```

   *Nota: Los comandos pueden variar para otros sistemas operativos (Windows, macOS).*
5. **Ubicar la Ruta de Java Home:**

   ```bash
   readlink -f $(which java)
   ```

   Este comando proporcionar√° la ruta completa de su instalaci√≥n de Java, la cual ser√° necesaria en el siguiente paso.
6. **Configurar la Variable de Entorno `JAVA_HOME`:**

   Edite su archivo de configuraci√≥n del shell (por ejemplo, `.bashrc` para Linux o `.zshrc` para Zsh/PowerShell) y a√±ada las siguientes l√≠neas al final:

   ```bash
   # Ejemplo con nano para .zshrc
   nano ~/.zshrc
   ```

   A√±ada al final del archivo:

   ```bash
   export JAVA_HOME="[ruta_obtenida_en_el_paso_anterior]"
   export PATH=$JAVA_HOME/bin:$PATH
   ```

   *Aseg√∫rese de reemplazar `[ruta_obtenida_en_el_paso_anterior]` con la ruta real que obtuvo en el paso 5.*
7. **Aplicar los Cambios al Shell:**

   ```bash
   source ~/.zshrc # O el archivo de configuraci√≥n de su shell
   ```
8. **Verificar la Instalaci√≥n de PySpark:**

   ```bash
   python -c "from pyspark.sql import SparkSession; print(SparkSession.builder.master('local[*]').getOrCreate().version)"
   ```

   Si la configuraci√≥n es correcta, ver√° una salida similar a esta (puede haber advertencias de WARN):

   ```
   25/04/23 00:55:28 WARN Utils: Your hostname, NicoNavarro resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
   25/04/23 00:55:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
   Setting default log level to "WARN". To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
   25/04/23 00:55:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
   3.4.1
   ```
9. **(Opcional) Configuraci√≥n de Variables de Entorno:**
   Si utiliza variables de entorno para rutas, puertos, etc., cree un archivo `.env` basado en el `.env.example` proporcionado en el repositorio.
10. **Iniciar el Contenedor de SQL Server:**

    ```bash
    cd sql_server
    docker-compose up -d
    ```
11. **Levantar los Notebooks para Desarrollo:**

    ```bash
    jupyter lab  # O utilice VSCode con la extensi√≥n de Jupyter
    ```

Al final ejecutando desde spark_docker: `docker compose --env-file ../.env up --build` construir√° los contenedores y dejar√° la base de datos lista para utilizarse y conectarse a ella mediante cualquier IDE, en mi caso utilic√© Dbeaver:

![1748932554716](image/README/1748932554716.png)

# Modelo de Datos Propuesto

El modelo de datos implementado sigue una estructura de **copo de nieve (snowflake)**, una elecci√≥n adecuada para entornos que requieren una alta normalizaci√≥n de las dimensiones. Esto permite mantener los datos limpios, estructurados y f√°cilmente escalables para futuras necesidades anal√≠ticas.

El modelo se centra en la tabla de hechos `fact_invoices`, que captura la granularidad de cada l√≠nea de factura (producto vendido por cliente en una fecha espec√≠fica). Esta tabla se relaciona con diversas tablas de dimensi√≥n: `dim_customer`, `dim_product`, `dim_time`, y sus respectivas jerarqu√≠as normalizadas.

#### Diagrama del Modelo

![1748898058963](image\README\1748914384449.png)

#### Tablas del Modelo

A continuaci√≥n, se detalla la estructura de las tablas que componen el Data Warehouse:

##### Tabla de Hechos: `fact_invoices`

| Columna          | Tipo         | Descripci√≥n                                           |
| :--------------- | :----------- | :----------------------------------------------------- |
| `id`           | `INT`      | Identificador √∫nico de la transacci√≥n (PK)           |
| `id_customer`  | `INT`      | Cliente que realiz√≥ la compra (FK a `dim_customer`) |
| `id_product`   | `INT`      | Producto vendido (FK a `dim_product`)                |
| `id_date`      | `INT`      | Fecha de la factura (FK a `dim_time`)                |
| `quantity`     | `INT`      | N√∫mero de unidades vendidas                           |
| `total_amount` | `FLOAT`    | Valor total de la transacci√≥n                         |
| `created_at`   | `DATETIME` | Fecha de creaci√≥n del registro                        |
| `updated_at`   | `DATETIME` | √öltima actualizaci√≥n del registro                    |
| `deleted_at`   | `DATETIME` | Marca de eliminaci√≥n l√≥gica (si aplica)              |

![1748930526068](image/README1/1748930526068.png)

![1748930692585](image/README1/1748930692585.png)

##### Tablas de Dimensi√≥n:

###### **`dim_customer`**

| Columna         | Tipo         | Descripci√≥n                                      |
| :-------------- | :----------- | :------------------------------------------------ |
| `id`          | `INT`      | Identificador √∫nico del cliente (PK)             |
| `name`        | `TEXT`     | Nombre completo del cliente                       |
| `id_location` | `INT`      | Localizaci√≥n del cliente (FK a `dim_location`) |
| `id_segment`  | `INT`      | Segmento del cliente (FK a `dim_segment`)       |
| `created_at`  | `DATETIME` | Fecha de creaci√≥n                                |
| `updated_at`  | `DATETIME` | Fecha de actualizaci√≥n                           |
| `deleted_at`  | `DATETIME` | Eliminaci√≥n l√≥gica                              |

![1748930624055](image/README1/1748930624055.png)****

###### **`dim_location`**

| Columna        | Tipo         | Descripci√≥n                                   |
| :------------- | :----------- | :--------------------------------------------- |
| `id`         | `INT`      | Identificador √∫nico de regi√≥n/localidad (PK) |
| `name`       | `TEXT`     | Nombre de la regi√≥n                           |
| `created_at` | `DATETIME` | Fecha de creaci√≥n                             |
| `updated_at` | `DATETIME` | Fecha de actualizaci√≥n                        |
| `deleted_at` | `DATETIME` | Eliminaci√≥n l√≥gica                           |

![1748930831527](image/README1/1748930831527.png)

###### **`dim_segment`**

| Columna         | Tipo         | Descripci√≥n                                      |
| :-------------- | :----------- | :------------------------------------------------ |
| `id`          | `INT`      | Identificador √∫nico del segmento de cliente (PK) |
| `description` | `TEXT`     | Descripci√≥n del segmento                         |
| `created_at`  | `DATETIME` | Fecha de creaci√≥n                                |
| `updated_at`  | `DATETIME` | Fecha de actualizaci√≥n                           |
| `deleted_at`  | `DATETIME` | Eliminaci√≥n l√≥gica                              |

![1748930922719](image/README1/1748930922719.png)

![1748930939382](image/README1/1748930939382.png)

###### **`dim_product`**

| Columna         | Tipo         | Descripci√≥n                                            |
| :-------------- | :----------- | :------------------------------------------------------ |
| `id`          | `INT`      | Identificador √∫nico del producto (PK)                  |
| `name`        | `TEXT`     | Nombre del producto                                     |
| `id_category` | `INT`      | Categor√≠a del producto (FK a `dim_product_category`) |
| `price`       | `FLOAT`    | Precio unitario del producto                            |
| `created_at`  | `DATETIME` | Fecha de creaci√≥n                                      |
| `updated_at`  | `DATETIME` | Fecha de actualizaci√≥n                                 |
| `deleted_at`  | `DATETIME` | Eliminaci√≥n l√≥gica                                    |

![1748931003248](image/README1/1748931003248.png)

![1748931015189](image/README1/1748931015189.png)

###### **`dim_product_category`**

| Columna        | Tipo         | Descripci√≥n                               |
| :------------- | :----------- | :----------------------------------------- |
| `id`         | `INT`      | Identificador √∫nico de la categor√≠a (PK) |
| `name`       | `TEXT`     | Nombre de la categor√≠a                    |
| `created_at` | `DATETIME` | Fecha de creaci√≥n                         |
| `updated_at` | `DATETIME` | Fecha de actualizaci√≥n                    |
| `deleted_at` | `DATETIME` | Eliminaci√≥n l√≥gica                       |

![1748931055635](image/README1/1748931055635.png)

![1748931068791](image/README1/1748931068791.png)

###### **`dim_date`**

| Columna         | Tipo     | Descripci√≥n                          |
| :-------------- | :------- | :------------------------------------ |
| `date_id`     | `INT`  | Identificador √∫nico de la fecha (PK) |
| `full_date`   | `DATE` | Fecha completa                        |
| `day`         | `INT`  | D√≠a del mes                          |
| `month`       | `INT`  | Mes num√©rico                         |
| `month_name`  | `TEXT` | Nombre del mes                        |
| `quarter`     | `INT`  | Trimestre (1-4)                       |
| `year`        | `INT`  | A√±o calendario                       |
| `attribute_8` | `TEXT` | Campo reservado para extensiones      |

![1748931202596](image/README1/1748931202596.png)

![1748931178684](image/README1/1748931178684.png)

#### Granularidad

La tabla de hechos `fact_invoices` tiene una granularidad a nivel de l√≠nea de venta por producto, cliente y fecha. Esta granularidad permite un an√°lisis detallado de los patrones de consumo, segmentado por cliente, categor√≠a de producto y regi√≥n geogr√°fica.

#### Consultas de An√°litica

Las consultas se realizaron directamente en la base de datos y por cuestiones de tiempo en implementaci√≥n no se logr√≥ mostrar el resultado de las queries como tablas o vistas en **gold_prod.bi_innova.resultados_queries.** Sin embargo se propone implementar **dbt** o simplemente la configuraci√≥n actual de ejecuci√≥n de queries almacenando los resultados en el catalogo para BI.

##### ¬øCu√°l es el producto m√°s vendido en cada trimestre del a√±o?: Resultado en

```
data_lake/gold/bi_innova/top_selling_product_by_quarter.csv
```

```sql
WITH ranked_products AS (

  SELECT
    d.[year],
    d.quarter,
    p.product_name,
    SUM(f.quantity) AS total_quantity,
    ROW_NUMBER() 
      OVER (
        PARTITION BY d.[year], d.quarter 
        ORDER BY SUM(f.quantity) DESC
      ) AS rn
  FROM dwh_innova.fact_invoices AS f
    JOIN dwh_innova.dim_date AS d
      ON f.id_date = d.date_id
    JOIN dwh_innova.dim_product AS p
      ON f.id_product = p.id_product
  GROUP BY
    d.[year],
    d.quarter,
    p.product_name
)
SELECT
  [year],
  quarter,
  product_name,
  total_quantity
FROM ranked_products
WHERE rn = 1
```

![1748931788015](image/README1/1748931788015.png)

##### ¬øCu√°les son las tendencias de compra de los clientes m√°s leales?

```sql
WITH top_clients AS (
  SELECT TOP 5
    c.id_customer,
    c.customer_name,
    SUM(f.total_amount) AS annual_spent
  FROM dwh_innova.fact_invoices AS f
    JOIN dwh_innova.dim_date AS d
      ON f.id_date = d.date_id
    JOIN dwh_innova.dim_customer AS c
      ON f.id_customer = c.id_customer
  WHERE d.[year] = 2024
  GROUP BY
    c.id_customer,
    c.customer_name
  ORDER BY
    SUM(f.total_amount) DESC
)
SELECT
  d.[year],
  d.month,
  tc.customer_name,
  SUM(f.total_amount) AS monthly_spent
FROM dwh_innova.fact_invoices AS f
  JOIN dwh_innova.dim_date AS d
    ON f.id_date = d.date_id
  JOIN top_clients AS tc
    ON f.id_customer = tc.id_customer
GROUP BY
  d.[year],
  d.month,
  tc.customer_name
ORDER BY
  tc.customer_name,
  d.[year],
  d.month;

```

![1748932085822](image/README1/1748932085822.png)

##### ¬øC√≥mo var√≠an las ventas seg√∫n las regiones geogr√°ficas durante el a√±o?

```sql
SELECT
  d.[year],
  d.month,
  l.city             AS region,
  SUM(f.total_amount) AS total_sales
FROM dwh_innova.fact_invoices AS f
  JOIN dwh_innova.dim_date     AS d
    ON f.id_date = d.date_id
  JOIN dwh_innova.dim_customer AS c
    ON f.id_customer = c.id_customer
  JOIN dwh_innova.dim_location AS l
    ON c.id_location = l.id_location
WHERE
  d.[year] = 2023
GROUP BY
  d.[year],
  d.month,
  l.city
ORDER BY
  d.month,
  l.city;

```

![1748932173189](image/README1/1748932173189.png)

Las respuestas pueden mejorar dependiendo del detalle o nivel que el cliente desee en su an√°lisis. Sin embargo las presentadas son utiles y funcionales para un abordaje inicial.

![1748932173189](image/README1/1748932173189.png)Estructura del Repositorio `alegra/`

A continuaci√≥n se detalla la estructura del repositorio:

```txt
.
‚îú‚îÄ‚îÄ data_lake/                  # Data Lake estructurado por capas (Bronze/Silver)
‚îÇ   ‚îú‚îÄ‚îÄ bronze/innova/          # Archivos Parquet crudos por entidad y fecha de carga
‚îÇ   ‚îú‚îÄ‚îÄ silver/innova/          # Archivos Parquet transformados (staging) para SQL Server
|   ‚îî‚îÄ‚îÄ gold/innova/            # Archivos Parquet listos para consumo (modelado final, agregaciones)

‚îú‚îÄ‚îÄ docs/                       # Documentaci√≥n del modelo l√≥gico
‚îÇ   ‚îú‚îÄ‚îÄ images/                 # Diagramas, como el modelo de datos en PNG
‚îÇ   ‚îî‚îÄ‚îÄ logical_model/          # Exportaci√≥n del modelo l√≥gico (PowerDesigner u otro)

‚îú‚îÄ‚îÄ data_source/                # Archivos con la data original (prescindible al conectarlo a data en producci√≥n)

‚îú‚îÄ‚îÄ metadata/                  # Archivos JSON con metadatos por paso del ETL
‚îÇ   ‚îî‚îÄ‚îÄ innova/
‚îÇ       ‚îú‚îÄ‚îÄ step_01__ingest_raw.json
‚îÇ       ‚îî‚îÄ‚îÄ step_02__transform_and_load_stg.json

‚îú‚îÄ‚îÄ spark_docker/              # Entorno Dockerizado
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml     # Levanta Spark, SQL Server y Jupyter
‚îÇ   ‚îî‚îÄ‚îÄ jupyter/Dockerfile     # Imagen base con Java 11, PySpark 3.4.1

‚îú‚îÄ‚îÄ src/innova/                # L√≥gica principal de ETL y carga al DWH
‚îÇ   ‚îú‚îÄ‚îÄ config/                # Configuraciones y DDL para SQL Server
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ddl/               # Scripts de creaci√≥n de dimensiones y hechos
‚îÇ   ‚îú‚îÄ‚îÄ etl/                   # Notebooks que ejecutan cada paso del flujo
‚îÇ   ‚îú‚îÄ‚îÄ sql/                   # Scripts de MERGE y consultas anal√≠ticas (Gold Layer)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gold_analitycs/    # Consultas BI: ventas por regi√≥n, productos top, etc.
‚îÇ   ‚îú‚îÄ‚îÄ utils/                 # Orquestador principal `DataWarehouseLoader`
‚îÇ   ‚îî‚îÄ‚îÄ populate_dim_date.ipynb # Notebook para cargar `dim_time`

‚îú‚îÄ‚îÄ utils/                     # Funciones auxiliares y conexi√≥n a SQL Server
‚îÇ   ‚îú‚îÄ‚îÄ connections/           # BaseConnector y SQLServerConnector
‚îÇ   ‚îú‚îÄ‚îÄ spark_helpers.py       # Inicializaci√≥n y funciones comunes para Spark
‚îÇ   ‚îú‚îÄ‚îÄ trackers.py            # Logging y tracking de metadatos
‚îÇ   ‚îî‚îÄ‚îÄ transform_schema.py    # Casters seg√∫n esquemas definidos

‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md
```

## La estructura interna de este repositorio consta de:

### üìÅ `data_source/`: Archivos fuente originales

Esta carpeta contiene los archivos CSV que simulan el origen transaccional de los datos. Se trata de tres archivos con estructura fija y predefinida:

* `customers.csv`
* `products.csv`
* `invoices.csv`

Estos archivos se utilizan √∫nicamente como entrada para el pipeline ETL. No se conectan a un motor SQL real ni a una API externa. En la etapa de ingesta (`step_01`), estos datos se copian directamente al √°rea **bronze** del Data Lake sin alteraciones, garantizando trazabilidad y separaci√≥n de responsabilidades.

---

### üìÅ `data_lake/`: Almacenamiento por capas (Bronze, Silver, Gold)

Esta carpeta implementa la estructura del Data Lake bajo el enfoque cl√°sico de **multizonas**:

#### üü§ `bronze/`

Contiene los datos **crudos** en formato Parquet, directamente derivados de los CSV fuente. Se respeta su estructura original y se particionan por `load_date` para garantizar trazabilidad de las cargas.

* Ubicaci√≥n t√≠pica: `data_lake/bronze/innova/<tabla>/load_date=<timestamp>/`
* Archivos: Parquet `.snappy`, `_SUCCESS`, archivos `.crc` (generados por Spark).

#### ‚ö™ `silver/`

Aqu√≠ se almacenan los datos **transformados** y **normalizados** listos para ser cargados al Data Warehouse. Esta zona representa la capa de *staging* y sigue el esquema l√≥gico definido en los scripts SQL.

* Ubicaci√≥n: `data_lake/silver/innova/temp_tables/<tabla>_stg/`
* Cambios aplicados:

  * Casting de tipos.
  * Estandarizaci√≥n de nombres de columnas.
  * Limpieza de registros nulos o inconsistentes.

#### üü° `gold/`

Contiene las **vistas anal√≠ticas y m√©tricas** requeridas por la prueba t√©cnica, listas para consumo por herramientas de BI o consultas de negocio.

* Ubicaci√≥n: `data_lake/gold/innova/<vista>`
* Aqu√≠ se encuentran outputs de queries como:

  * Productos m√°s vendidos por trimestre.
  * Tendencias de compras por cliente.
  * Ventas por regi√≥n.

Esta estructura permite evolucionar f√°cilmente hacia esquemas m√°s robustos con integraci√≥n a cat√°logos externos o Delta Lake.

---

### üìÅ `metadata/`: Trazabilidad y control del pipeline

Esta carpeta almacena archivos JSON generados autom√°ticamente por el pipeline ETL en cada uno de sus pasos. Estos archivos contienen **metadatos esenciales** como:

* Ruta de almacenamiento de cada entidad procesada.
* Timestamp de carga (`load_date`).
* N√∫mero total de registros procesados.

#### üìÑ Ejemplos:

* `step_01__ingest_raw.json`: generado tras la ingesta a Bronze.
* `step_02__transform_and_load_stg.json`: generado tras la transformaci√≥n a Silver.

Estos archivos son consumidos por etapas posteriores del pipeline para:

* Evitar hardcodeos de rutas.
* Validar volumenes de datos entre etapas.
* Registrar logs estructurados del proceso.

Este enfoque modular y trazable demuestra una **arquitectura robusta y escalable**, propia de entornos reales de producci√≥n.

---

### üìÅ `spark_docker/`: Entorno reproducible con Docker

Este directorio contiene la infraestructura necesaria para levantar el entorno completo de desarrollo de manera aislada, sin depender de servicios externos. Se compone de:

#### üìÑ `docker-compose.yml`

Define los servicios que componen el stack t√©cnico del proyecto:

* **Spark + Jupyter**: nodo maestro con PySpark 3.4.1, Java 11 y JupyterLab como interfaz.
* **SQL Server**: contenedor que emula una base de datos OLTP para almacenar el Data Warehouse.

Este archivo permite levantar todo con:

```bash
docker-compose up -d
```

#### üìÑ `jupyter/Dockerfile`

Construye la imagen base del contenedor Spark. Incluye:

* Python 3.10
* Java 11
* Apache Spark 3.4.1
* Paquetes requeridos (`pyspark`, `pyodbc`, etc.)

Esto permite ejecutar notebooks PySpark de forma local, conect√°ndose a SQL Server dentro del mismo entorno.

---

### üìÅ `src/innova/config/`: Configuraci√≥n y definici√≥n del modelo f√≠sico

Contiene los archivos necesarios para parametrizar el pipeline y definir la estructura del Data Warehouse en SQL Server.

* `config.py`: centraliza rutas, esquemas y nombres de tablas usados en el pipeline.
* `ddl/`: colecci√≥n de scripts `.sql` que crean las tablas del modelo tipo **copo de nieve** (dimensiones y hechos). Estos scripts son ejecutados autom√°ticamente durante la carga al DWH.

#### üìÑ `etl_config.yml`: Configuraci√≥n declarativa del pipeline ETL

Este archivo YAML representa el coraz√≥n de la parametrizaci√≥n del proyecto. Define:

* Las **capas del Lakehouse** (`bronze`, `silver`, `gold`).
* Las **rutas f√≠sicas** de trabajo: data lake, logs, metadata y fuentes.
* El  **orden y habilitaci√≥n de pasos ETL** , permitiendo orquestaci√≥n din√°mica.
* El  **esquema detallado de cada tabla fuente** , especificando tipos estrictos.
* La  **l√≥gica de modelado DWH** , incluyendo mapeos de columnas, claves naturales y relaciones for√°neas (`fk_map`).

Este enfoque permite desacoplar la l√≥gica de implementaci√≥n del flujo ETL de su configuraci√≥n, facilitando la extensibilidad hacia nuevos negocios sin tocar el c√≥digo fuente.

---

### üìÅ `src/innova/etl/`: Pipeline ETL modular con PySpark

Esta carpeta contiene los notebooks que implementan cada etapa del pipeline. Cada notebook es independiente, recibe sus par√°metros desde `etl_config.yml`, y deja trazabilidad en forma de metadatos (`metadata/*.json`) y logs.

Contiene los archivos necesarios para parametrizar el pipeline y definir la estructura del Data Warehouse en SQL Server.

* `config.py`: centraliza rutas, esquemas y nombres de tablas usados en el pipeline.
* `ddl/`: colecci√≥n de scripts `.sql` que crean las tablas del modelo tipo **copo de nieve** (dimensiones y hechos). Estos scripts son ejecutados autom√°ticamente durante la carga al DWH.

#### üìÑ `etl_config.yml`: Configuraci√≥n declarativa del pipeline ETL

Este archivo YAML representa el coraz√≥n de la parametrizaci√≥n del proyecto. Define:

* Las **capas del Lakehouse** (`bronze`, `silver`, `gold`).
* Las **rutas f√≠sicas** de trabajo: data lake, logs, metadata y fuentes.
* El  **orden y habilitaci√≥n de pasos ETL** , permitiendo orquestaci√≥n din√°mica.
* El  **esquema detallado de cada tabla fuente** , especificando tipos estrictos.
* La  **l√≥gica de modelado DWH** , incluyendo mapeos de columnas, claves naturales y relaciones for√°neas (`fk_map`).

Este enfoque permite desacoplar la l√≥gica de implementaci√≥n del flujo ETL de su configuraci√≥n, facilitando la extensibilidad hacia nuevos negocios sin tocar el c√≥digo fuente.

#### üìì `stp_01__ingest_data.ipynb`: Ingesta cruda a Bronze

Este notebook realiza la ingesta inicial de los archivos CSV desde `data_source/`, y los guarda en la zona **bronze** del Data Lake en formato Parquet, respetando su estructura original.

* Lee configuraci√≥n din√°mica de tablas desde `etl_config.yml`.
* Crea particiones `load_date=<timestamp>` por entidad (`customers`, `products`, `invoices`).
* Genera el archivo de metadata `step_01__ingest_raw.json` con ruta y conteo de registros.

**Objetivo:** preservar los datos fuente sin alteraciones para garantizar trazabilidad y facilitar reprocesos.

#### üìì `stp_02__load_stg_tables.ipynb`: Transformaci√≥n y carga a Staging

Este paso lee los datos de Bronze, los transforma y los deja listos en la base de datos SQL Server como tablas de staging.

* Aplica filtros y casting estricto seg√∫n los esquemas definidos en el config.
* Escribe los datos transformados en:

  * Parquet (zona `silver/temp_tables`).
  * SQL Server (`silver_prod.temp_tables.*`) con conexi√≥n v√≠a `SQLServerConnector`.
* Registra el proceso en `step_02__transform_and_load_stg.json`.

**Objetivo:** estandarizar los datos y prepararlos para alimentar el modelo dimensional del DWH.

#### üìì `stp_03__make_dwh.ipynb`: Construcci√≥n del modelo de datos

Este notebook ejecuta el proceso de carga del Data Warehouse, basado en el modelo copo de nieve propuesto.

* Lee los datos staging desde `step_02__transform_and_load_stg.json`.
* Usa la clase `DataWarehouseLoader` para:

  * Ejecutar los scripts DDL de creaci√≥n de tablas (`dim_*`, `fact_*`).
  * Resolver claves for√°neas.
  * Hacer `MERGE` (upserts) desde staging a las tablas del DWH.
* Se apoya en configuraci√≥n desde `etl_config.yml`.

**Objetivo:** materializar el modelo de datos completo en SQL Server y garantizar su consistencia referencial.

#### üìì `stp_04__make_bi.ipynb`: Generaci√≥n de vistas anal√≠ticas (Gold)

Notebook final del pipeline que prepara la capa Gold con consultas anal√≠ticas derivadas del modelo DWH.

* Configura conexiones y rutas necesarias.
* Las vistas o tablas resultantes se generan mediante los scripts SQL en `src/innova/sql/gold_analitycs/`.
* Incluye an√°lisis como:

  * Clientes leales por volumen de compras.
  * Productos m√°s vendidos por trimestre.
  * Tendencias regionales de ventas.

**Objetivo:** facilitar la lectura de m√©tricas de negocio solicitadas en la prueba, listas para consumo por analistas o herramientas BI.

---

### üìÅ `src/innova/sql/`: L√≥gica SQL del Data Warehouse y capa anal√≠tica

Esta carpeta contiene todos los scripts SQL que complementan el pipeline PySpark. Se dividen en dos grupos:

#### Scripts de carga al DWH

Ubicados en la ra√≠z de esta carpeta, estos scripts ejecutan operaciones `MERGE` para poblar las tablas del modelo copo de nieve desde las tablas staging (`silver_prod.temp_tables.*`).

| Script                                        | Descripci√≥n                                                           |
| --------------------------------------------- | ---------------------------------------------------------------------- |
| `dwh_innova_dim_customer_merge.sql`         | Inserta y actualiza clientes en `dim_customer`.                      |
| `dwh_innova_dim_location_merge.sql`         | Pobla `dim_location`.                                                |
| `dwh_innova_dim_segment_merge.sql`          | Inserta segmentos √∫nicos de cliente.                                  |
| `dwh_innova_dim_product_merge.sql`          | Carga productos, resolviendo la categor√≠a asociada.                   |
| `dwh_innova_dim_product_category_merge.sql` | Crea o actualiza categor√≠as en `dim_product_category`.              |
| `dwh_innova_fact_invoices_merge.sql`        | Carga la tabla de hechos `fact_invoices`, resolviendo todas las FKs. |

Todos estos scripts son ejecutados autom√°ticamente desde el notebook `stp_03__make_dwh.ipynb` mediante la clase `DataWarehouseLoader`.

#### `gold_analitycs/`: Consultas anal√≠ticas (capa Gold)

Contiene los scripts que generan vistas anal√≠ticas directamente sobre el modelo DWH. Estas consultas responden a los KPIs solicitados en la prueba.

| Script                                  | M√©trica o pregunta respondida                                    |
| --------------------------------------- | ----------------------------------------------------------------- |
| `loyal_customers_purchase_trends.sql` | ¬øCu√°les son los clientes m√°s leales y qu√© productos compran?  |
| `sales_by_region_trends.sql`          | ¬øC√≥mo var√≠an las ventas seg√∫n la regi√≥n a lo largo del a√±o? |
| `top_selling_product_by_quarter.sql`  | ¬øCu√°l es el producto m√°s vendido por trimestre?                |

Estas consultas pueden desplegarse como vistas materializadas o servir como insumo para dashboards de negocio.

---

### üìÅ `src/innova/utils/loader.py`: Orquestador de carga al Data Warehouse

Este archivo implementa la clase `DataWarehouseLoader`, n√∫cleo del paso `step_03`, responsable de construir y poblar el modelo de datos en SQL Server.

#### ¬øQu√© hace?

* Ejecuta la carga completa del **modelo copo de nieve**, resolviendo la secuencia correcta de inserci√≥n.
* Controla la inserci√≥n de dimensiones simples, dimensiones con claves for√°neas, y la tabla de hechos.
* Usa archivos SQL externos (`*_merge.sql`) para ejecutar upserts v√≠a `MERGE`.

#### Caracter√≠sticas t√©cnicas:

* **Carga topol√≥gica:** calcula autom√°ticamente el orden correcto de carga entre entidades, respetando dependencias de claves for√°neas.
* **Soporte para staging:** usa tablas temporales (`stg_*`) para staging interno antes de hacer `MERGE`.
* **Resuelve FKs din√°micamente:** realiza joins contra dimensiones ya cargadas o consultadas desde SQL.
* **Cache de dimensiones:** evita recargas innecesarias gracias a un sistema de cache interno (`_dim_cache`).
* **Dise√±o desacoplado:** toma la configuraci√≥n desde `etl_config.yml`, permitiendo definir cada entidad (select, natural keys, FKs, SQL path) sin tocar el c√≥digo.

#### Flujo de ejecuci√≥n (m√©todo `run()`):

1. Determina orden de carga (`_topological_order`).
2. Carga primero dimensiones *lookup* (sin FKs).
3. Luego dimensiones dependientes (`_resolve_fk`).
4. Finalmente, carga la tabla de hechos (`fact_invoices`) usando la misma l√≥gica.

---

### üìì `populate_dim_date.ipynb`: Generaci√≥n de la dimensi√≥n de tiempo (`dim_time`)

Este notebook construye y carga la dimensi√≥n `dim_date` en el esquema `dwh_innova`, abarcando el a√±o 2023. Es una tabla auxiliar indispensable en cualquier modelo dimensional, ya que permite realizar an√°lisis temporales (por mes, trimestre, fin de semana, etc.).

### üìÅ `utils/`: Utilidades compartidas del pipeline ETL

Este m√≥dulo agrupa componentes reutilizables que soportan toda la ejecuci√≥n del pipeline. Permiten desacoplar la l√≥gica de negocio de aspectos t√©cnicos como configuraci√≥n, casting de esquemas, conexiones a bases de datos, logging y ejecuci√≥n Spark.

Incluye funciones puras, conectores, utilitarios para casting estricto de tipos y carga din√°mica de configuraci√≥n, todos usados en m√∫ltiples etapas (`step_01` a `step_04`) para garantizar consistencia y trazabilidad.

#### üìÑ `utils/config.py`: Cargador centralizado de configuraci√≥n

Este m√≥dulo define una √∫nica funci√≥n: `load_etl_config`, que se encarga de leer el archivo `etl_config.yml` correspondiente a un negocio determinado (por ejemplo, `innova`).

* Busca el archivo en la ruta: `src/<business>/config/etl_config.yml`.
* Carga el contenido como diccionario Python utilizando `yaml.safe_load`.
* Si el archivo no existe, lanza un error expl√≠cito.

#### üìÑ `utils/spark_helpers.py`: Utilidades para manejo de Spark y archivos Parquet

Este m√≥dulo proporciona funciones auxiliares para facilitar tareas comunes en Spark durante la ejecuci√≥n del pipeline, especialmente en las etapas de ingesta y transformaci√≥n.

#### ¬øQu√© ofrece?

* **Lectura y escritura de archivos Parquet:**

  * `load_parquet`: carga segura de archivos Parquet, con logging y manejo de errores.
  * `save_parquet`: guarda un `DataFrame` con control de modo (`overwrite`) y partici√≥n.
* **Carga y escritura de metadatos:**

  * `load_metadata` / `save_metadata`: lectura y persistencia de archivos JSON que contienen informaci√≥n clave por paso del pipeline (rutas, conteos, timestamps).
* **Ejecuci√≥n de consultas SQL desde archivo:**

  * `execute_query`: ejecuta una query guardada en disco como archivo `.sql` usando Spark SQL, incluyendo logging detallado.
* **Registro de vistas temporales:**

  * `register_temp_view`: permite exponer un `DataFrame` como vista temporal en Spark para consultas encadenadas.

**Utilidad:**
Estas funciones abstraen operaciones repetitivas, aseguran consistencia en el manejo de errores y facilitan el desarrollo de notebooks m√°s limpios y reutilizables.

#### üìÑ `utils/trackers.py`: Logging estructurado y gesti√≥n de metadatos

Este m√≥dulo facilita el seguimiento estructurado del pipeline mediante logs persistentes y generaci√≥n autom√°tica de archivos de metadatos por paso.

#### ¬øQu√© incluye?

* **`setup_logging(log_path)`**
  Inicializa un logger de nivel `INFO` que escribe en un archivo fijo (`etl.log`). Asegura que los logs de ejecuci√≥n queden registrados y reutiliza una √∫nica instancia de logger para todo el pipeline.
* **`log_and_register(...)`**
  Componente clave durante la ingesta (`step_01`):

  * Cuenta registros.
  * Guarda el `DataFrame` como Parquet.
  * Actualiza el diccionario de metadatos con `path`, `load_date` y `record_count`.
* **`save_metadata(...)`**
  Persiste el diccionario generado en el paso anterior como archivo `.json` en la carpeta `metadata/`, nombrado seg√∫n el nombre del paso (`step_01`, `step_02`, etc.).

**Utilidad:**
Este archivo centraliza todo el control de **auditor√≠a y trazabilidad** del pipeline, permitiendo mantener registros consistentes por paso, detectar errores y facilitar debugging o reintentos parciales.

#### üìÑ `utils/transform_schema.py`: Aplicaci√≥n de esquemas estrictos

Este m√≥dulo define la funci√≥n `apply_strict_schema`, utilizada durante el paso de transformaci√≥n (`step_02`) para garantizar que cada columna del `DataFrame` tenga el tipo de dato correcto antes de ser persistido o cargado en staging.

#### ¬øQu√© hace?

* Recibe un `DataFrame` y un diccionario de esquema (`schema_dict`) que define el tipo esperado de cada columna.
* Castea expl√≠citamente cada columna a:

  * `StringType`
  * `LongType`
  * `IntegerType`
  * `DateType` (en formato `"yyyy-MM-dd"`)
  * `DecimalType(precision, scale)`
* Permite extenderse f√°cilmente para soportar m√°s tipos Spark (`BooleanType`, `TimestampType`, etc.).

**Utilidad:**
Esta funci√≥n asegura que los datos cumplen con la estructura esperada del modelo dimensional antes de ser cargados al DWH, evitando errores en tiempo de ejecuci√≥n y asegurando integridad en la persistencia.

---

#### üìÑ `utils/connections/base_connector.py`: Interfaz abstracta para conectores de bases de datos

Este archivo define la clase `BaseConnector`, una clase abstracta que sirve como contrato base para cualquier conector de base de datos que se desee implementar en el pipeline.

#### ¬øQu√© contiene?

Una interfaz con los siguientes m√©todos abstractos:

* `connect()`: inicializa la conexi√≥n.
* `close()`: cierra y limpia los recursos de la conexi√≥n.
* `execute_sql(sql, options)`: ejecuta una consulta SQL y retorna un `DataFrame`.
* `write_dataframe(df, table, mode, options)`: escribe un `DataFrame` en una tabla destino, configurable en modo (`overwrite`, `append`, etc.).

**Utilidad:**
Este dise√±o permite implementar conectores espec√≠ficos para distintos motores (SQL Server, PostgreSQL, Snowflake, etc.) cumpliendo con la misma interfaz, lo que facilita el mantenimiento y la extensi√≥n del pipeline en contextos reales.

#### üìÑ `utils/connections/sql_server_connector.py`: Conector h√≠brido a SQL Server

Este archivo implementa la clase `SQLServerConnector`, una extensi√≥n de `BaseConnector`, que facilita la interacci√≥n con SQL Server tanto para lectura como escritura de datos.

#### ¬øQu√© ofrece?

* **Autoconfiguraci√≥n flexible:**

  * Puede tomar los par√°metros de conexi√≥n desde variables de entorno (`SQLSERVER_HOST`, etc.) o un diccionario `config`.
  * Soporta encriptaci√≥n y configuraci√≥n segura por defecto.
* **Conexi√≥n h√≠brida (PySpark + pyodbc):**

  * Lectura de datos como `DataFrame` v√≠a `Spark JDBC`.
  * Ejecuci√≥n de DDL/DML v√≠a `pyodbc`, √∫til para scripts `MERGE`, `DROP`, etc.
* **Operaciones clave:**

  * `execute_sql(...)`: ejecuta consultas como `SELECT` (Spark) o `INSERT`, `UPDATE`, `MERGE` (pyodbc).
  * `write_dataframe(...)`: escribe un `DataFrame` a una tabla destino mediante JDBC.
  * `drop_table(...)`: elimina tablas si existen.
  * `upsert(...)`: funci√≥n de alto nivel que:

    1. Carga datos en tabla `staging`.
    2. Lee plantilla SQL `MERGE`.
    3. Ejecuta la l√≥gica `MERGE` en destino utilizando `sp_executesql`.

**Utilidad:**
Este conector simplifica y unifica el manejo de datos con SQL Server, permitiendo realizar cargas completas, actualizaciones, validaciones y pruebas desde notebooks PySpark con una sola clase reutilizable.

---

## Reflexi√≥n Final y Valor de esta Soluci√≥n

Este proyecto no solo cumple con los requisitos t√©cnicos de la prueba, sino que tambi√©n demuestra una **visi√≥n realista de implementaci√≥n en entornos productivos**.

Se implement√≥ una arquitectura completa que simula un entorno de tipo Lakehouse, incluyendo:

* **Procesamiento distribuido real** con PySpark.
* **Persistencia y trazabilidad** con metadatos estructurados y logs detallados.
* **Cargas incrementales al DWH** usando SQL Server real en contenedor.
* **Modularidad y escalabilidad**, con soporte multinegocio desde la ra√≠z del pipeline.

Adem√°s de resolver correctamente los puntos solicitados, se adoptaron **pr√°cticas de ingenier√≠a de datos robustas**, como separaci√≥n por capas (`bronze/silver/gold`), abstracciones reutilizables, versionado de estructuras (`DDL`) y automatizaci√≥n de procesos cr√≠ticos (`MERGE`, `upsert`, etc.).

---

Esta soluci√≥n est√° pensada no solo como una respuesta a un reto t√©cnico, sino como una **base reutilizable y adaptable a m√∫ltiples escenarios empresariales reales**.
Gracias por revisar este trabajo ‚Äî ¬°quedo atento a cualquier mejora o desaf√≠o futuro!
